 Summary:
The dataset was split into training (60%), validation (20%), and test (20%) sets to ensure reliable evaluation. Four models—Logistic Regression, Decision Tree, Random Forest, and XGBoost—were implemented, with XGBoost achieving the highest performance (Accuracy: 0.91, F1-Score: 0.91). Random Forest was further tuned using Grid Search, slightly improving accuracy and F1-score on the test set. Feature importance analysis of tree-based models highlighted the top predictors, while learning curves indicated low bias and moderate variance, confirming good generalization. Overall, XGBoost and tuned Random Forest provided the best predictive performance, with interpretable insights for key features.